{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/zephyr-7B-beta-GGUF/resolve/main/zephyr-7b-beta.Q4_K_M.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"./llm/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./input.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"./sample.json\", \"r\") as f:\n",
    "    samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_str(data):\n",
    "    input_str = [\"Dialogue:\"]\n",
    "    for d in data[\"dialogue\"]:\n",
    "        utter = f\"{d['speaker']}: {d['utterance']}\"\n",
    "        input_str.append(utter)\n",
    "    summary = f\"Summary:\\n{data['summary']}\"\n",
    "    input_str.append(summary)\n",
    "\n",
    "    return \"\\n\".join(input_str)\n",
    "\n",
    "def generate_sample_str(data):\n",
    "    input_str = generate_input_str(data)\n",
    "    return f\"{input_str}\\nAnswer:\\n{data['answer']}\\n\"\n",
    "\n",
    "def generate_zeroshot_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_fewshot_prompt(samples, input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "\n",
    "{generate_input_str(samples[0])}\n",
    "Answer: Yes\n",
    "\n",
    "{generate_input_str(samples[1])}\n",
    "Answer: No\n",
    "\n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_zeroshot_cot_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "\n",
    "Answer:\n",
    "Let's think step by step\"\"\"\n",
    "\n",
    "def generate_fewshot_cot_prompt(samples, input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "\n",
    "{generate_sample_str(samples[0])}\n",
    "{generate_sample_str(samples[1])}\n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_fewshot_cot_prompt(samples, input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "\n",
    "{generate_sample_str(samples[0])}\n",
    "{generate_sample_str(samples[1])}\n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_analogical_prompt(input):\n",
    "    return f\"\"\"Your will be give a dialogue and a summary. Your task is to determine whether the summary accurately reflects the dialogue or not. When presented with the dialogue and summary, recall relevant problems as examples. Afterward, proceed with the answer.\n",
    "# Problem:\n",
    "{generate_input_str(input)}\n",
    "\n",
    "# Instructions:\n",
    "## Relevant Problems:\n",
    "Recall two examples of dialogues and summaries that are relevant to the initial problem. The examples should be distinct from each other and from the initial problem. For each sample:\n",
    "- After \"Q: \", describe the dialogue and summary\n",
    "- After \"A: \", explain the solution and provide your answer with either 'Yes' or 'No'.\n",
    "## Solve the initial problem:\n",
    "Q: Copy and paste the initial problem here.\n",
    "A: Explain the solution and provide your answer with either 'Yes' or 'No'.\n",
    "\"\"\"\n",
    "\n",
    "def generate_ps_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "\n",
    "Answer:\n",
    "Let's first understand the problem and devise a plan to solve the problem. Then let's carry out the plan and solve the problem step by step\"\"\"\n",
    "\n",
    "def generate_psplus_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "\n",
    "Answer: \n",
    "Let's first read and understand the dialogue carefully, extract facts from each of the dialogue utterances, read the summary, and last compare your understandings of the summary with facts from the dialogue. Devise a plan to answer the instruction, then carry out the plan to solve the problem step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Yes, I can help you with that! Here's my plan to determine whether the summary accurately reflects the dialogue:\n",
      "Step 1: Read and understand the dialogue carefully\n",
      "I have read the dialogue provided and understood each utterance made by Grace and Henry.\n",
      "Step 2: Extract facts from each of the dialogue utterances\n",
      "Grace: How are we allocating the budget for this project?\n",
      "Henry: We should allocate more funds to the advertising segment.\n",
      "Facts extracted from the dialogue:\n",
      "* Grace asked how the budget for the project is being allocated.\n",
      "* Henry suggested that more funds should be allocated to the advertising segment.\n",
      "Step 3: Read the summary and compare it with the facts extracted from the dialogue\n",
      "The summary states, \"Henry suggested the team find more funds for the project.\"\n",
      "Comparison of the summary with the facts extracted from the dialogue:\n",
      "* The summary does not accurately reflect the dialogue. Henry did not suggest finding more funds for the project as a whole, but rather allocated more funds to the advertising segment within the project.\n",
      "Therefore, the answer is \"No\". The summary does not accurately reflect the dialogue."
     ]
    }
   ],
   "source": [
    "prompt = generate_psplus_prompt(generate_input_str(data[1]))\n",
    "\n",
    "response_iter = llm.stream_complete(prompt)\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nHow many helicopters can a human eat in one sitting?</s>\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nHow many helicopters can a human eat in one sitting?</s>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
