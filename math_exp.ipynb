{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/math/input.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"./data/math/sample.json\", \"r\") as f:\n",
    "    samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zeroshot_prompt(input):\n",
    "    return f\"\"\"Q: {input['question']}\n",
    "A: \"\"\"\n",
    "\n",
    "def generate_fewshot_prompt(samples, input):\n",
    "    return f\"\"\"Q: {samples[0]['question']}\n",
    "A: {samples[0]['answer']}\n",
    "\n",
    "Q: {samples[1]['question']}\n",
    "A: {samples[1]['answer']}\n",
    "\n",
    "Q: {input['question']}\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "def generate_zeroshot_cot_prompt(input):\n",
    "    return f\"\"\"Q: {input['question']}\n",
    "A: Let's think step by step. \"\"\"\n",
    "\n",
    "def generate_fewshot_cot_prompt(samples, input):\n",
    "    return f\"\"\"Q: {samples[0]['question']}\n",
    "A: \n",
    "{samples[0]['explanation']}\n",
    "\n",
    "Q: {samples[1]['question']}\n",
    "A: \n",
    "{samples[1]['explanation']}\n",
    "\n",
    "Q: {input['question']}\n",
    "A: \n",
    "\"\"\"\n",
    "\n",
    "def generate_analogical_prompt(input):\n",
    "    return f\"\"\"Your task is to tackle mathematical problems. When presented with a math problem, recall relevant problems as examples. Afterward, proceed to solve the initial problem.\n",
    "# Problem:\n",
    "{input['question']}\n",
    "\n",
    "# Instructions:\n",
    "## Relevant Problems:\n",
    "Recall three examples of math problems that are relevant to the initial problem. Your problems should be distinct from each other and from the initial problem (e.g., involving different numbers and names). For each problem:\n",
    "- After \"Q: \", describe the problem\n",
    "- After \"A: \", explain the solution and highlight the final answer.\n",
    "\n",
    "## Solve the Initial Problem:\n",
    "Q: Copy and paste the initial problem here.\n",
    "A: Explain the solution and highlight the final answer.\n",
    "\"\"\"\n",
    "\n",
    "def generate_ps_prompt(input):\n",
    "    return f\"\"\"Q: {input['question']}\n",
    "A: Let's first understand the problem and devise a plan to solve the problem. Then let's carry out the plan and solve the problem step by step. \"\"\"\n",
    "\n",
    "def generate_psplus_prompt(input):\n",
    "    return f\"\"\"Q: {input['question']}\n",
    "A: Let's first read and understand the problem carefully, extract numbers from the list that fulfill the condition provided, then compute the numbers according to the instruction. Devise a plan to answer the instruction, then carry out the plan to solve the problem step by step. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_path=\"./../simple-rag/llm/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=2048,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  5%|▌         | 1/20 [05:15<1:39:52, 315.39s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 10%|█         | 2/20 [09:41<1:25:58, 286.58s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 15%|█▌        | 3/20 [16:04<1:33:38, 330.51s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 20%|██        | 4/20 [21:21<1:26:43, 325.20s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 25%|██▌       | 5/20 [27:13<1:23:44, 334.95s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 30%|███       | 6/20 [31:50<1:13:31, 315.14s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 35%|███▌      | 7/20 [36:52<1:07:19, 310.75s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 40%|████      | 8/20 [42:43<1:04:45, 323.77s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 45%|████▌     | 9/20 [47:28<57:07, 311.61s/it]  Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 50%|█████     | 10/20 [54:22<57:10, 343.01s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 55%|█████▌    | 11/20 [1:00:42<53:09, 354.35s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 60%|██████    | 12/20 [1:05:26<44:24, 333.11s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 65%|██████▌   | 13/20 [1:12:50<42:47, 366.76s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 70%|███████   | 14/20 [1:18:26<35:43, 357.25s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 75%|███████▌  | 15/20 [1:24:41<30:13, 362.76s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 80%|████████  | 16/20 [1:30:56<24:25, 366.30s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 85%|████████▌ | 17/20 [1:36:36<17:55, 358.33s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 90%|█████████ | 18/20 [1:40:40<10:48, 324.09s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 95%|█████████▌| 19/20 [1:47:31<05:50, 350.31s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "100%|██████████| 20/20 [1:53:01<00:00, 339.06s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"mistral-7b-instruct\"\n",
    "os.makedirs(f\"./results/math/{model}\", exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    \"zeroshot\": [],\n",
    "    \"fewshot\": [],\n",
    "    \"zeroshot_cot\": [],\n",
    "    \"fewshot_cot\": [],\n",
    "    \"analogical\": [],\n",
    "    \"ps\": [],\n",
    "    \"psplus\": [],\n",
    "}\n",
    "\n",
    "for d in tqdm(data):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_zeroshot_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"zeroshot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_fewshot_prompt(samples, d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"fewshot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_zeroshot_cot_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"zeroshot_cot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_fewshot_cot_prompt(samples, d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"fewshot_cot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_analogical_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"analogical\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_ps_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"ps\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_psplus_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"psplus\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "for prompt_type, result in results.items():\n",
    "    with open(f\"./results/math/{model}/{prompt_type}.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_path=\"./../simple-rag/llm/llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=2048,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  5%|▌         | 1/20 [04:52<1:32:44, 292.87s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 10%|█         | 2/20 [12:15<1:54:21, 381.17s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 15%|█▌        | 3/20 [17:38<1:40:25, 354.45s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 20%|██        | 4/20 [24:57<1:43:28, 388.01s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 25%|██▌       | 5/20 [30:43<1:33:10, 372.70s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 30%|███       | 6/20 [37:12<1:28:15, 378.25s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 35%|███▌      | 7/20 [42:16<1:16:41, 353.95s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 40%|████      | 8/20 [48:19<1:11:23, 356.98s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 45%|████▌     | 9/20 [53:55<1:04:11, 350.17s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 50%|█████     | 10/20 [1:01:30<1:03:46, 382.62s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 55%|█████▌    | 11/20 [1:06:08<52:35, 350.56s/it]  Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 60%|██████    | 12/20 [1:12:23<47:44, 358.12s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 65%|██████▌   | 13/20 [1:17:37<40:12, 344.61s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 70%|███████   | 14/20 [1:24:12<35:59, 359.86s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 75%|███████▌  | 15/20 [1:29:37<29:06, 349.35s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 80%|████████  | 16/20 [1:36:28<24:31, 367.82s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 85%|████████▌ | 17/20 [1:41:56<17:48, 356.06s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 90%|█████████ | 18/20 [1:48:03<11:58, 359.21s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 95%|█████████▌| 19/20 [1:53:36<05:51, 351.54s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "100%|██████████| 20/20 [1:58:42<00:00, 356.12s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"llama2-7b-chat\"\n",
    "os.makedirs(f\"./results/math/{model}\", exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    \"zeroshot\": [],\n",
    "    \"fewshot\": [],\n",
    "    \"zeroshot_cot\": [],\n",
    "    \"fewshot_cot\": [],\n",
    "    \"analogical\": [],\n",
    "    \"ps\": [],\n",
    "    \"psplus\": [],\n",
    "}\n",
    "\n",
    "for d in tqdm(data):\n",
    "    prompt = generate_zeroshot_prompt(d)\n",
    "    results[\"zeroshot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "    \n",
    "    prompt = generate_fewshot_prompt(samples, d)\n",
    "    results[\"fewshot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = generate_zeroshot_cot_prompt(d)\n",
    "    results[\"zeroshot_cot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = generate_fewshot_cot_prompt(samples, d)\n",
    "    results[\"fewshot_cot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = generate_analogical_prompt(d)\n",
    "    results[\"analogical\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = generate_ps_prompt(d)\n",
    "    results[\"ps\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "    prompt = generate_psplus_prompt(d)\n",
    "    results[\"psplus\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text,\n",
    "        \"gold_answer\": d[\"answer\"]\n",
    "    })\n",
    "\n",
    "for prompt_type, result in results.items():\n",
    "    with open(f\"./results/math/{model}/{prompt_type}.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCPP(\n",
    "    model_path=\"./../simple-rag/llm/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=2048,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
