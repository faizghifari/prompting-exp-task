{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/input.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open(\"./data/sample.json\", \"r\") as f:\n",
    "    samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_str(data):\n",
    "    input_str = [\"Dialogue:\"]\n",
    "    for d in data[\"dialogue\"]:\n",
    "        utter = f\"{d['speaker']}: {d['utterance']}\"\n",
    "        input_str.append(utter)\n",
    "    summary = f\"Summary:\\n{data['summary']}\"\n",
    "    input_str.append(summary)\n",
    "\n",
    "    return \"\\n\".join(input_str)\n",
    "\n",
    "def generate_sample_str(data):\n",
    "    input_str = generate_input_str(data)\n",
    "    return f\"{input_str}\\nAnswer:\\n{data['answer']}\\n\"\n",
    "\n",
    "def generate_zeroshot_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_fewshot_prompt(samples, input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. Answer with \"Yes\" or \"No\".\n",
    "\n",
    "{generate_input_str(samples[0])}\n",
    "Answer: Yes\n",
    "\n",
    "{generate_input_str(samples[1])}\n",
    "Answer: No\n",
    "\n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_zeroshot_cot_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "\n",
    "Answer:\n",
    "Let's think step by step\"\"\"\n",
    "\n",
    "def generate_fewshot_cot_prompt(samples, input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. Answer with \"Yes\" or \"No\".\n",
    "\n",
    "{generate_sample_str(samples[0])}\n",
    "{generate_sample_str(samples[1])}\n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_analogical_prompt(input):\n",
    "    return f\"\"\"Your will be given a dialogue and a summary. Your task is to determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. When presented with the dialogue and summary, recall relevant dialogues and summaries as examples. Afterward, proceed with the answer \"Yes\" or \"No\".\n",
    "\n",
    "# Instructions:\n",
    "## Relevant Problems:\n",
    "Recall two examples of dialogues and summaries that are relevant to the initial problem. The examples should be distinct from each other and from the initial problem. For each sample:\n",
    "- After \"Dialogue: \", describe the dialogue\n",
    "- After \"Summary: \", describe the summary\n",
    "- After \"Answer: \", explain the solution on determining whether the summary accurately reflects the dialogue or not.\n",
    "## Solve the initial problem:\n",
    "{generate_input_str(input)}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def generate_ps_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "\n",
    "Answer:\n",
    "Let's first understand the problem and devise a plan to solve the problem. Then let's carry out the plan and solve the problem step by step\"\"\"\n",
    "\n",
    "def generate_psplus_prompt(input):\n",
    "    return f\"\"\"Given the data consists of dialogue and summary below, determine whether the summary accurately reflects the dialogue or not. A summary is accurate when all of its content are consistent with the dialogue. In contrast, if there is a part of the summary that is inconsistent or not supported by the dialogue, even if its only one part, then the summary is inaccurate. Answer with \"Yes\" or \"No\".\n",
    "    \n",
    "{generate_input_str(input)}\n",
    "\n",
    "Answer: \n",
    "Let's first read and understand the dialogue carefully, extract facts from each of the dialogue utterances, read the summary, and last compare your understandings of the summary with facts from the dialogue. Devise a plan to answer the instruction, then carry out the plan to solve the problem step by step.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    model_path=\"./../simple-rag/llm/zephyr-7b-beta.Q4_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=2048,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "  5%|▌         | 1/20 [02:53<55:02, 173.84s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 10%|█         | 2/20 [11:36<1:53:37, 378.77s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 15%|█▌        | 3/20 [27:50<3:04:22, 650.76s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 20%|██        | 4/20 [38:57<2:55:14, 657.19s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 25%|██▌       | 5/20 [47:38<2:31:59, 607.94s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 30%|███       | 6/20 [56:16<2:14:43, 577.38s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 35%|███▌      | 7/20 [1:11:36<2:29:21, 689.36s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 40%|████      | 8/20 [1:22:41<2:16:18, 681.58s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 45%|████▌     | 9/20 [1:31:30<1:56:14, 634.07s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 50%|█████     | 10/20 [1:34:49<1:23:16, 499.63s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 55%|█████▌    | 11/20 [1:38:08<1:01:08, 407.65s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 60%|██████    | 12/20 [1:47:13<59:54, 449.34s/it]  Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 65%|██████▌   | 13/20 [2:01:39<1:07:10, 575.78s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 70%|███████   | 14/20 [2:16:01<1:06:12, 662.16s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 75%|███████▌  | 15/20 [2:27:29<55:49, 669.92s/it]  Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 80%|████████  | 16/20 [2:42:14<48:58, 734.61s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 85%|████████▌ | 17/20 [2:56:51<38:52, 777.44s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 90%|█████████ | 18/20 [3:07:32<24:33, 736.56s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      " 95%|█████████▌| 19/20 [3:13:26<10:21, 621.57s/it]Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "100%|██████████| 20/20 [3:23:44<00:00, 611.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = \"zephyr-7b-beta\"\n",
    "os.makedirs(f\"./results/verify_summary_v2/{model}\", exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    \"zeroshot\": [],\n",
    "    \"fewshot\": [],\n",
    "    \"zeroshot_cot\": [],\n",
    "    \"fewshot_cot\": [],\n",
    "    \"analogical\": [],\n",
    "    \"ps\": [],\n",
    "    \"psplus\": [],\n",
    "}\n",
    "\n",
    "for d in tqdm(data):\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_zeroshot_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"zeroshot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_fewshot_prompt(samples, d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"fewshot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_zeroshot_cot_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"zeroshot_cot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_fewshot_cot_prompt(samples, d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"fewshot_cot\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_analogical_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"analogical\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_ps_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"ps\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": f\"{generate_psplus_prompt(d)}\"}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    results[\"psplus\"].append({\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": llm.complete(prompt).text\n",
    "    })\n",
    "\n",
    "for prompt_type, result in results.items():\n",
    "    with open(f\"./results/verify_summary_v2/{model}/{prompt_type}.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "for fpath in glob.glob(\"./results/verify_summary_v1/*/*.json\"):\n",
    "    with open(fpath, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "    \n",
    "    found = False\n",
    "    for idx, r in enumerate(result):\n",
    "        try:\n",
    "            if (r[\"is_accurate\"] == r[\"predict\"]) != r[\"verdict\"]:\n",
    "                found = True\n",
    "                print(idx)\n",
    "                break\n",
    "        except:\n",
    "            raise KeyError(fpath)\n",
    "    if found:\n",
    "        print(fpath)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verdict = {}\n",
    "\n",
    "for dir in glob.glob(\"./results/verify_summary_v2/*/\"):\n",
    "    model_type = dir.split(\"\\\\\")[1]\n",
    "    if \"beta\" not in model_type:\n",
    "        verdict = {}\n",
    "        for fpath in glob.glob(f\"{dir}*.json\"):\n",
    "            prompt_type = fpath.split(dir)[-1].split(\".\")[0]\n",
    "            with open(fpath, \"r\") as f:\n",
    "                result = json.load(f)\n",
    "            \n",
    "            TP = FN = FP = TN = EXP = NO_ANS = PLAN = SAMPLES = 0\n",
    "            for r in result:\n",
    "                if r[\"is_accurate\"] == True and r[\"predict\"] == True:\n",
    "                    TP += 1\n",
    "                elif r[\"is_accurate\"] == True and r[\"predict\"] == False:\n",
    "                    FN += 1\n",
    "                elif r[\"is_accurate\"] == False and r[\"predict\"] == True:\n",
    "                    FP += 1\n",
    "                elif r[\"is_accurate\"] == False and r[\"predict\"] == False:\n",
    "                    TN += 1\n",
    "                \n",
    "                if r[\"explanation\"]: EXP += 1\n",
    "                if r[\"no_answer\"]: NO_ANS += 1\n",
    "                if r[\"plan_and_steps\"]: PLAN += 1\n",
    "                if r[\"relevant_samples\"]: SAMPLES += 1\n",
    "\n",
    "            verdict[prompt_type] = {\n",
    "                \"TP\": TP,\n",
    "                \"FN\": FN,\n",
    "                \"FP\": FP,\n",
    "                \"TN\": TN,\n",
    "                \"accuracy\": ((TP + TN) / (TP + FN + FP + TN)) * 100.0,\n",
    "                \"explanation\": EXP,\n",
    "                \"no_answer\": NO_ANS,\n",
    "                \"plan_and_steps\": PLAN,\n",
    "                \"relevant_samples\": SAMPLES\n",
    "            }\n",
    "        \n",
    "        all_verdict[model_type] = verdict\n",
    "\n",
    "with open(\"./results/verify_summary_v2/verdict.json\", \"w\") as f:\n",
    "    json.dump(all_verdict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
